---
title: "Notes: 'Hands-On Machine Learning with R'"
author: "Steven Marcel Bißantz"
date: "2023-03-25"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Disclaimer**: The script contains my extended summary of the book "Hands-On
Machine Learning with R" by Bradley Boehmke & Brandon Greenwell. That is, most
of the information you read here is not mine. Nevertheless, I have also
reworded sections and incorporated my own thoughts. For a clear delimitation,
please read the original text.

## Chapter 1: Introduction 

**Machine learning**: Develop algorithms/models that use information, such as
data or prior knowledge, to approximate or extract functional relationships
between variables.

**Learning**: …means adaption; i.e. the learner uses the data (or evidence) to
(self) optimize the algorithmic steps (e.g., the  parameters, $\theta$, of a
model) to make accurate prediction.

*Example*: In supervised learning we adapt to a known (“labeled”) target. In
unsupervised machine learning we need to adapt to an unknown (“unlabeled”)
target.

**Loss function**: …but ‘accuracy’ in terms of what? Our goal is encapsulated
in a loss (or utility) function, that specifies the difference to a target.

*Example*: The Kullback-Leibler Divergence minimizes the (info) distance
between our model distribution $\mathbf{q}$ and the true frequency distribution
of events $\mathbf{p}$

**Optimization**: With the loss function we optimize (often: minimize, rarely:
maximize) a (often: loss, rarely: utility) function using an optimization
algorithm, like stochastic gradient descent (SGD) algorithm. The algorithm
tells us at each iteration the difference to a prespecified optimum (often: the
minimum, e.g. 0).

```{r, echo=FALSE}
if (knitr:::is_latex_output()) {
  knitr::asis_output('https://upload.wikimedia.org/wikipedia/commons/a/a3/Gradient_descent.gif')
} else {
  knitr::include_graphics("./homl_files/sgd.gif")
}
```
Figure: An intuitive visualization of the stochastic gradient descent method.
The picture is taken from:
https://commons.wikimedia.org/wiki/File:Gradient_descent.gif  

*Side note*: If AIML someday manages to learn causal relationships among
variables (DAGs), it might be a data-driven replacement for theory development.

### "Mere" predictions” vs. "causal predictions"

*In a nutshell*: Mere predictions aim to accurately predict outcomes, causal
predictions aim to estimate the effects of interventions and understand the
causal relationships between input features and the outcome variable (see:
DAGs).

"Mere" predictions focus on accurately predicting the outcome of a target
variable based on input features without necessarily considering the underlying
causes or mechanisms that drive the relationship between them. 

*Example*: This type of prediction is commonly used in applications such as
recommender systems, image classification, and natural language processing.

**Causal predictions**: ...on the other hand, aim to estimate the effect of an
intervention or treatment on a target variable by accounting for the causal
relationships between the input features and the outcome variable. This
requires a deeper understanding of the underlying causal mechanisms and
potential confounding factors that can impact the outcome. 

*Example*: Causal inference techniques such as randomized controlled trials,
propensity score matching, instrumental variable analysis and, often forgoten,
*do-calculus* given observational data, are commonly used to estimate causal
effects in various domains such as healthcare, social sciences, and economics.

#### Machine learning paradigms

We could classify ML models according to the amount of supervision needed during
training. However, do not forget reinforcement learning:

1. Supervised learning (~predictive models)
2. Unsupervised learning (–descriptive models)
3. Reinforcement Learning

**Supervision**: …means, to provide the algorithm with predefined “labeled” training
data. No supervision implies, the algorithm needs to do the assignment of the
labels itself.

**Labelling**: …refers to the process of assigning a specific category or value
to each data point (e.g., based on experts opinion, or a specific criterion)

*Example*: Experts diagnosed (i.e. “labeled”) 500 patients with Major
Depression Disorder (MDD): $y$. Now we can train an algorithm based on these
data, $\mathcal{D}_{train}:\{X,y\}_{i=1}^{n_{train}}$, together with patient
characteristics, $X: f(X) = y$. Then we can use $f$, the learner, to make
predictions for new, unseen patients, for example, $i$ in
$\mathcal{D}_{test}:\{X,y\}_{i=1}^{n_{test}}$.

#### Data quality & Validity 

I have seen few authors discussing this point: *Do always critically reflect on
the quality of your labels*. In unsupervised machine learning we often do this,
because we want to check the quality of the machine predictions. In supervised
machine learning the labels are preassigned. Still, we must check the validity
of the scores assigned by experts or criteria. Do they make sense or are they
high quality?

The quality of the approximation ($y = f(X)$) is only of importance if $y$ is
valid. If $y$ is trash then we get “a good approximation – but of trash”.
*Garbage in garbage out* it is that simple.

### Supervised learning (~predictive models)

**Supervised learning**: Develop an algorithm or predictive model that uses
labeled data $D:\{X,y\}_{i=1}^n$ to produces an accurate (e.g., non- or
piecewise linear) approximation of the mapping function “$f$”:  $y=f(X)$ -- and
make accurate predictions for future values of $\tilde{y}$ based on a set of
features ($X$). (*Note*: The labels are often assigned based on a criterion or
expert knowledge)

**Why is supervised learning “supervised”?**

Because the *training* data ($\mathcal{D}_{train}:\{X,y\}_{i=1}^{n_{train}}$)
you feed the algorithm includes the target values ($y$). Consequently, the
solutions can be used to help supervise the training process to find the
optimal algorithm parameters.

**Underlying function assumption**: Given a dataset comprised of inputs and
outputs $D:\{X,y\}_{i=1}^n$, we assume that there is an unknown underlying
function that is consistent in mapping inputs to outputs in the target domain
and resulted in the dataset. We then use supervised learning algorithms to
approximate this function.

*Open Question*: Does it need to be a function, or are also relations possible?
e.g. $y = x^2$?. I think relations are not possible, since there is no unique
element in the codomain or range for each element in the domain.

#### The two problems in supervised learning

1. Regression problem 
2. Classification problem

#### Regression problem

**Regression problem**: …means the objective of our supervised learning is to
predict a numeric outcome that falls on a continuum $(-\infty, \infty)$.

$$ y_{\in \mathbb{R}} = f(X_{\in \mathbb{R}}) $$

*Example*: Predict height based on persons characteristics, like sex.

#### Classification problem

**Clasification problem**: …means the objective of our supervised learning is
to predict a categorical outcome (i.e., a binary or multinomial response
measure)

*Example*: Classify customer reviews from 0 to 5 (multinomial) or “like”,
“don't like” (binary).

*Important*: In ML often ordinal responses are modeled as categorical processes
and thus with a multinomial distribution. 

**Classification problems are kind of regression problems**: Every
classification is a (logistic) regression problem. In a general regression
problem can be formalized as $y_{\in \mathbb{R}} = f(X_{\in \mathbb{R}})$. In a
classification problem the situation is as follows: $y_{\in [0,1]} = f(X_{\in
\mathbb{R}})$. *Note that $[0,1]$ is a subset of $\mathbb{R}^+$*. The problem
is thus: $y_{\in [0,1]\subset \mathbb{R}^+} = f(X_{\in \mathbb{R}})$. What
about the categories? In this binary case, the labels for the categories are
assigned ex post according to 0/1 assignment rule.

**0/1 Assignment rule**: If we predict the probability of a class ($P(X=1)$);
by default, the class with the highest predicted probability becomes the
predicted class:

$$ k = \begin{cases} 0 & \Leftrightarrow p < 0.5 \\ 1 & \Leftrightarrow p \geq
0.5 \end{cases} $$

### Unsupervised learning (~descriptive models)

**Unsupervised learning**: ...develop algorithms that uses unlabeled
information to discover functional relationships between variables (rows,
columns). 

In essence, unsupervised learning is concerned with identifying groups in a
data set. The groups may be defined by the rows (i.e., clustering) or the
columns (i.e., dimension reduction); however, the motive in each case is quite
different.

**Underlying function assumption**: Given a dataset: $D:\{Z_i\}$, we assume
that there is an unknown functional relationship between the variables (rows,
columns). We then use unsupervised learning algorithms to discover this
function.

#### The two topics in unsupervised learning:

1. Clustering (~ rows in the data set)
2. Dimension reduction (~ columns in the data set)

#### Clustering

**Clustering**: …means to use the observed variables (i.e., columns) to segment
observations (i.e., rows) into similar (more homogeneous) groups.

*Example*: “Market segmentation” -- where we divide consumers into different
homogeneous groups.

#### Dimension reduction

**Dimension reduction**: …means to reduce the number of variables (i.e.
columns) in a data set.

*Example*: Factor analysis -- identify the underlying dimensions (factors) that
explain patterns of correlations among multiple variables (items).

$$R^* =  \Lambda \Phi \Lambda + \Psi$$

##### Full compression vs. meaningful compression
Full -- orthogonal -- compression versus meaningful compression:

**Full compression**: If we have a regression model which tend to break with
highly correlated variables, or have a data set where the predictors are so
rich that we can't process them with our machines, we can use PCA to compress
the data set fully, i.e. reduce it with minimal loss in recovery to the
smallest amount of uncorrelated variables -- the principal components.

**Meaningful compression**:  If we want to understand or model the structural
relationships between variables with more flexibility (i.e. $\Phi$) we can try
to find factors that first and foremost make sense.

#### Downstream supervised learning

**Downstream supervised learning**: The outputs of unsupervised learning models
can be used as inputs to downstream supervised learning models.

*Example*: Using a reduced feature set of a PCA and input it to downstream
supervised learning models (e.g., principal component regression).

##### Problems: Quality assessment in supervised and unsupervised learning

With unsupervised learning it seems harder to assess the quality of the results,
because we don't know the true answer — the problem is unsupervised!

With predictive models and supervised learning algorithms (i.e., linear
regression), it is possible to check our work by seeing how well our model
predicts the response Y on observations not used in fitting the model.

However, this does not safeguard against the quality check for the labels. If
those are trash, then good predictions are meaningless.

### Reinforcement learning

**Reinforcement learning**: ...is a training method based on rewarding desired
behaviors and/or punishing undesired ones. In general, a reinforcement learning
agent is able to perceive and interpret its environment, take actions and learn
through trial and error.

## Chapter 2: Modeling Process

```{r}
# Helper packages
pkgs <- c("rsample", # Resampling procedures
          "caret",   # Resampling and model training 
          "h20")     # Resampling and model training
           
# lapply(pkgs, library, character.only=TRUE) 

# h2o 
h2o::h2o.no_progress() # Avoid the progress bars in the output
h2o::h2o.init()

?AmesHousing::ames_raw

# Ames housing data
ames <- AmesHousing::make_ames()

# Coerce to a h2o frame
ames_h2o <- h2o::as.h2o(ames)

# Job attrition data
# Note: the data are now in the modeldata package
churn <- modeldata::attrition
str(churn)
```

Important: h2o cannot handle ordered factors so we need to coerce them to 
unordered factors before the analysis.

```{r}
# Coerce ordered factors
unorder_if <- function(x) {
  # Have to call 'return()' explicitally
  ifelse(is.ordered(x), factor(x, ordered = FALSE), return(x))
  }
# Trick: Use '[]' to keep the data frame structure
churn[] <- lapply(churn, unorder_if)
str(churn)
```

**No free lunch theorem** (Wolpert 1996): "There is no single  model that can
outperform all other algorithms on all possible tasks or problems."

*Implication 1*: Participate in an iterative, playful and explorative model
building process. Never rely on a single "best" algorithm for all problems.
(see also: Stacking)

*Implication 2*: Approaching ML modeling correctly means approaching it
strategically by
* …spending our data wisely on learning and validation procedures,
* …properly pre-processing the feature and target variables,
* …minimizing data leakage,
* …tuning hyperparameters, and
* …assessing model performance

**Generalizability**: The ability of an algorithm to accurately predict future
outcomes, rather than just fitting well to past data, e.g. the sample.

### Data Splitting

To provide an accurate understanding of the generalizability of our final
optimal model, we can split our data into *training* and *test data sets*:

* **Training set**: The data we use to develop feature sets, train the
  algorithm, tune hyperparameters, and compare models.
* **Test set**: The data that we given a final model use to estimate an
  unbiased assessment of the model's performance on unseen data, i.e. its
  *generalization error*.

**Generalization error**: The difference between a model's performance on the
training data and its performance on new, unseen data, like the validation set.

#### Rough guidelines for the split

**N: small-medium**

* Spending too much in training (e.g., >80%) won't allow us to get a good
  assessment of predictive performance. We may find a model that fits the
  training data very well, but is not generalizable (*overfitting*).
* Sometimes too much spent in testing (>40%) won't allow us to get a good
  assessment of model parameters.

e.g. 60% (training)-40% (testing), 70%-30%, or 80%-20%

**N: big (100K)**

Small gains with additional data as compared to smaller sample sizes.

* $p \leq n$:  Use a smaller training sample to increase computation speed (e.g.,
  models built on larger training sets often take longer to score new data sets
  in production)
* $p \geq n$: Keep the samples sizes large, because the information is needed to
  identify consistent signals in the features.

#### Common splitting schemes:

1. Simple random sampling
2. Stratified sampling

#### Simple random sampling splitting

 **Simple random sampling splitting**: Take a simple random sample from the
 data -- with the desired probability

**Simple random sampling**: A sampling method where each member of the
population has an equal and independent chance of being selected for the
sample. This means that every possible sample of the desired size has an equal
probability of being selected. 

*Procedure*: Assing each member of the population a unique identification
number, and use a random number generator to select the required number of
samples. 

*Benefits*: ...considered to be unbiased and used to minimize the risk of
selection bias, where certain subgroups of the population are overrepresented
or underrepresented in the sample. 

*Problem*: Does not control for any data attributes, such as the distribution
of your response variable ($y$).

*Usecases*: Classification and regression problems when we have a sufficient
sample size. In classification problems especially when there is a good balance
between the classes.

#### Simple random sampling

Use `base` and no additional packages.

```{r}
# Reproduceable results
set.seed(123)
# Number of rows
n_rows <- nrow(ames)
# Row index
row_index <- seq(n_rows)
# Index for the training cases
train_index <- sample(row_index, size = round(n_rows) * 0.7, replace = FALSE)
# Training set
train <- ames[train_index,] ; cat("Training set: ", dim(train), "\n")
# Testing set
test <- ames[-train_index,] ; cat("Testing set: ", dim(test), "\n")
# Plausibility check
cat("Split: ", dim(train)[1] / n_rows * 100, "/", dim(test)[1] / n_rows * 100)
```

Use the `caret` package.

```{r}
set.seed(123)

# Reproduceable results
train_index <- caret::createDataPartition(ames$Sale_Price, p = 0.7, 
                               # Matrix instead of a list as output
                               list = FALSE)
# Training set
train <- ames[train_index,] ; cat("Training set: ", dim(train), "\n")
# Testing set
test <- ames[-train_index,] ; cat("Testing set: ", dim(test), "\n")
# Plausibility check
cat("Split: ", dim(train)[1] / n_rows * 100, "/", dim(test)[1] / n_rows * 100)
```

Use the `rsample` package.

```{r}
set.seed(123)  # for reproducibility
split  <- rsample::initial_split(ames, prop = 0.7)
# Training set
train <- rsample::training(split)
# Testing set
test <- rsample::testing(split)
# Plausibility check
cat("Split: ", dim(train)[1] / n_rows * 100, "/", dim(test)[1] / n_rows * 100)
```

#### Stratified sampling splitting

**Stratified sampling splitting**: Take a stratified sample from the data --
with the desired probability.

**Stratified sampling**: A sampling method where the population is divided into
subgroups (aka., *strata*), and then a random sample is taken from each
subgroup (aka. *stratum*) in proportion to their size or importance. 

*Procedure*: Segment $y$ into classes (discrete $y$) or quantiles (continuous
$y$) and randomly sample from each.

*Benefits*: Allows to explicitly control the sampling so that our training and
test sets have similar Y distributions

*Usecases*: Classification and regression problems

1. *Classification problems*: When $y$ is severely imbalanced (e.g., 90% “Yes” and
   10% “No”). See also: Class Imbalances 
2. *Regression problems*: With small sample size and where y deviates strongly
   from normality (i.e., positively skewed like Sale_Price).

```{r}
# Data with class imbalances 
table(churn$Attrition) |> prop.table()
```

Stratified sampling using the `rsample` package.

TODO

```{r}
set.seed(123)
split_strat <- rsample::initial_split(churn, prop = 0.7, strata = "Attrition")
train <- rsample::training(split_strat) 
test <- rsample::testing(split_strat) 
# Plausibility check
cat("Split: ", dim(train)[1] / n_rows * 100, "/", dim(test)[1] / n_rows * 100)
```








#### Class imbalances

Imbalanced data can have a significant impact on model predictions and
performance (Kuhn and Johnson 2013).

#### Categorization:

1. Up-sampling
2. Down-sampling
3. Over-Under-Sampling

#### Down sampling

**Down sampling**: Balance the dataset by reducing the size of the abundant
class(es) to match the frequencies in the least prevalent class.

*Important*: By keeping all samples in the rare class and randomly selecting
an equal number of samples in the abundant class, a balanced new dataset can be
retrieved for further modeling. Furthermore, the reduced sample size reduces
the computation burden imposed by further steps in the ML process.

**When should we use down-sampling**? Use If there are enough data.

#### Up-sampling

**Up-sampling**: A strateg where we try to balance the dataset by increasing
the size of rarer samples. 

*Important*: Rather than getting rid of abundant samples, new rare samples are
generated by using repetition or bootstrapping.

**When should we use up-sampling**? When the quantity of data is insufficient.

#### Over-Under-Sampling

**SMOTE**: A combination of over- and under-sampling is often successful. A
common approach is known as Synthetic Minority Over-Sampling Technique (SMOTE,
Chawla et al. 2002).

### Resampling methods

1. K-fold cross validation
2. Bootstrapping

*Note*: For other measures see the Anki card

#### K-fold cross validation

**K-fold cross validation**: A resampling method that randomly divides the
training data into k groups (aka., folds) of approximately equal size. The
model is fit on k-1 folds and then the remaining fold is used to compute
model performance. This procedure is repeated k times; each time, a different
fold is treated as the validation set. This process results in k estimates of
the generalization error.

Thus, the k-fold CV estimate is computed by averaging the k test errors,
providing us with an approximation of the error we might expect on unseen data.

Note: This is kind of the sampling distribution for the model performance

n practice, one typically uses k=5 or k=10. There is no formal rule as to the
size of k; however, as k gets larger, the difference between the estimated
performance and the true performance to be seen on the test set will decrease.
On the other hand, using too large k can introduce computational burdens.
Moreover, Molinaro, Simon, and Pfeiffer (2005) found that k=10 performed
similarly to leave-one-out cross validation (LOOCV) which is the most extreme
approach (i.e., setting k=n).

Although using k≥10 helps to minimize the variability in the estimated
performance, k-fold CV still tends to have higher variability than bootstrapping
(discussed next). Kim (2009) showed that repeating k-fold CV can help to
increase the precision of the estimated generalization error. Consequently, for
smaller data sets (say n<10,000), 10-fold CV repeated 5 or 10 times will improve
the accuracy of your estimated performance and also provide an estimate of its
variability

CV as you can often perform CV directly within certain ML functions:

Or externally as in the below chunk5. When applying it externally to an ML
algorithm as below, we’ll need a process to apply the ML model to each resample,
which we’ll also cover.

as.h2o(<my-data-frame>)

ordered factors and H2O has no way of handling this data type. Consequently, you
must convert any ordered factors to unordered; see ?base::ordered

see the sampling argument in ?caret::trainControl()

h2o algorithms have a weights_column and balance_classes

Many of these drawbacks and inconsistencies were originally organized and
presented by Kuhn (2018)

The caret package has been the preferred meta engine over the years; however,
the author is now transitioning to full-time development on parsnip, which is
designed to be a more robust and tidy meta engine.↩

### TODO Digression: Are BDA and ML separate things?

I really try to see ML algorithms as function approximation machines. Often,
they provide (smoothed) point-wise linear (e.g., DNNs) or non-linear (e.g.,
GPs) approximations of underlying (non-linear) functions.

However, for me it was hard to see how machine learning algorithms can be used
in probabilisitc modeling more generally and how they interplay with multilevel
models, for instance. I thought machine leanring and, what I call, "Bayesian
data" analysis are two different worlds. They are not. Both can be embedded in
framework of probabilisitc modeling; and thus we can combine then to produce
even more powerful models. 

*Example*: Combine GPs and Hierarchical modeling. One way to implement this
approach is to use a Bayesian hierarchical model, where the GP is used as the
link function for the regression model at each level. The parameters of the GP
are then estimated using a Bayesian approach, which allows for uncertainty
quantification in the predictions. Overall, using GPs to approximate the link
function in a multilevel model can provide a flexible and probabilistic
approach to modeling non-linear relationships between variables.

### Digression: Deep Neural Nets ~ Gaussian Processes

Lee, Bahri, et al. (2018) showed that, if the number or nodes in a carefully
scaled NN (with a single hidden layer) -- think of a MLP -- converges to
infinity, a DNN converges to a GP. However, the argument can be extended to
deeper layers by induction.

**GPs > DNN**: Consequently, *GPs only emulate non-linearity*. To put it
another way, GPs kind of resolve the parametric assumptions that MLPs make.
So I would prefer to use a GP over a MLP whenerver possible, even if your
sample size grows infinitely large. There are tricks to get it going; if not,
use a DNN.

**DNN > GPs**: Since the computational cost of GPs naively scales as
$\mathcal{O}(N^3)$, where $N$ are the data points, consider a MLP if a GP is
not possible with your data.

*Open Question*: I think this holds only for feed forward neural nets such as
MLPs. Recurrent Neural Networks (RNNs) and Convolutional Neural Networks
(CNNs), for instance, can be used to model time series data and image data,
respectively, and are not limited to pointwise linear function approximation.


~ Fourier series, Taylor expansion?

As many pointed out, a regression/decision tree is a non-linear model. Note
however that it is a piecewise linear model: in each neighborhood (defined in a
non-linear way), it is linear. In fact, the model is just a local constant.

Note: RF are tree based, while DNNs are gradient based.

https://machinelearningmastery.com/neural-networks-are-function-approximators/

Training a neural network on data approximates the unknown underlying mapping
function from inputs to outputs.

Is this why we can use a DNN to learn a link function?

Note: Deep Learning Is Not 'Worse' than Trees on Tabular Data
https://towardsdatascience.com/deep-learning-is-not-worse-than-trees-on-tabular-data-1de25ed31d2

--

(Non-)parametric

Nonparametric regression: is a category of regression analysis* in which the
predictor does not take a predetermined form but is constructed according to
information derived from the data. More specifically, the data supply the model
structure as well as the model estimates.

Q: is the number of parameters increasing as you process new training examples
during training? If so, the method is non-parametric.

Example: Gaussian process regression

*Regression analysis is a model family where the Data are divided in D:{X,y}
and we estimate the relationship “f” between X and y: y=f(X)

Are standard deep neural networks (DNN) parametric?

Technically speaking, yes, since they have a fixed number of parameters. (We
have to determine the model structure, I.e. layers, neurons, etc. beforehand)

Deep learning models are generally parametric - in fact they have a huge number
of parameters, one for each weight that is tuned during training. As the number
of weights generally stays constant, they technically have fixed degrees of
freedom. However, as there are generally so many parameters they may be seen to
emulate non-parametric.

Are Gaussian processes non-parametric?
Technically speaking, yes, since

GPs are non-parametric since it the number of parameter increase he number of
parameters increasing as you process new training examples – O(N^3)

Gaussian processes (for example) use each observation as a new weight and as
the number of points goes to infinity so too do the number of weights (not to
be confused with hyper parameters). I say generally because there are so many
different flavours of each model. For example low rank GPs have a bounded
number of parameters which are inferred by the data and I'm sure someone has
been making some type of non-parametric dnn at some research group!

—

Non-linear and non-parametric

(My idea – In a nutshell): in regression, non-linear refers to the shape of the
outcome y - variable, parametric to the way the X are combined.

—