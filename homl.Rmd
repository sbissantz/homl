---
title: "Notes: 'Hands-On Machine Learning with R'"
author: "Steven Marcel Bißantz"
date: "2023-03-25"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Disclaimer**: The script contains my extended summary of the book "Hands-On
Machine Learning with R" by Bradley Boehmke & Brandon Greenwell. That is, most
of the information you read here is not mine. Nevertheless, I have also
reworded sections and incorporated my own thoughts. For a clear delimitation,
please read the original text.

## Chapter 1: Introduction 

**Machine learning**: A framework for developing algorithmic models that use
information (e.g., data) to approximate or extract functional relationships
between variables.

**Learning**: The process by which the model integrates information (e.g.,
data) to optimize the algorithmic steps (e.g., parameters, weights) and thereby
improve performance on a given task (e.g., prediction for unseen data).

**Loss function**: An function that measures the discrepancy or error
between a model output (e.g., parameters, predictions) and a target - for *a
single* data instance.

*Example* (prediction loss): $L_{\text{sq}}(\hat y, y) = (\hat y- y)^2$

**Cost function** (aka. empirical risk): A function that measures the
discrepancy or error between a model output (e.g., parameters, predictions) and
a target - across (all) data instances.

Note: It is an aggregate of the Loss over the training, validation, or test
data \mathcal{D}= \left\{ (\mathbf{x}_i, y_i) \right\}_{i=1}^{n}

**Example** (Cost as the mean loss): $C_(f, \mathcal D) =
\frac{1}{n} \sum_{i=1}^n L(\hat{y}_i, y_i)$

**Training set**: A disjoint subset of the data set used to train the model,
which means to learn the model parameters

**Validation set**: An optional disjoint subset of the data used for model selection or to fine-tune the hyperparameters of the model (e.g. n° hidden layers in a DNN, learning rate) Note: Intermediary between training and testing sets. 

**Test set**: A disjoint subset of the data used to assess the performance of a trained machine learning model by evaluating its predictions on unseen data.
assesses model accuracy and generalization.

In some cases, cross-validation techniques can also be used to further split
the data and obtain better estimates of the model's performance.

### "Mere" predictions” vs. "causal predictions"

*In a nutshell*: Mere predictions aim to accurately predict outcomes, causal
predictions aim to estimate the effects of interventions and understand the
causal relationships between input features and the outcome variable (see:
DAGs).

"Mere" predictions focus on accurately predicting the outcome of a target
variable based on input features without necessarily considering the underlying
causes or mechanisms that drive the relationship between them. 

*Example*: This type of prediction is commonly used in applications such as
recommender systems, image classification, and natural language processing.

**Causal predictions**: ...on the other hand, aim to estimate the effect of an
intervention or treatment on a target variable by accounting for the causal
relationships between the input features and the outcome variable. This
requires a deeper understanding of the underlying causal mechanisms and
potential confounding factors that can impact the outcome. 

*Example*: Causal inference techniques such as randomized controlled trials,
propensity score matching, instrumental variable analysis and, often forgotten,
*do-calculus* given observational data, are commonly used to estimate causal
effects in various domains such as healthcare, social sciences, and economics.

#### Machine learning paradigms

We could classify ML models according to the amount of supervision needed during
training. However, do not forget reinforcement learning:

1. Supervised learning (~predictive models)
2. Unsupervised learning (–descriptive models)
3. Reinforcement Learning

**Supervision**: …means, to provide the algorithm with predefined “labeled” training
data. "No supervision" implies, the algorithm needs to do the assignment of the
labels itself.

**Labelling**: …refers to the process of assigning a specific category or value
to each data point (e.g., based on experts opinion, or a specific criterion)

*Example*: Experts diagnosed (i.e. “labeled”) 500 patients with Major
Depression Disorder (MDD): $y$. Now we can train an algorithm based on these
data, $\mathcal{D}_{train}:\{X,y\}_{i=1}^{n_{train}}$, together with patient
characteristics, $X: f(X) = y$. Then we can use $f$, the learner, to make
predictions for new, unseen patients, for example, $i$ in
$\mathcal{D}_{test}:\{X,y\}_{i=1}^{n_{test}}$.

#### Data quality & Validity 

I have seen few authors discussing this point: *Do always critically reflect on
the quality of your labels*. In unsupervised machine learning we often do this,
because we want to check the quality of the machine predictions. In supervised
machine learning the labels are preassigned. Still, we must check the validity
of the scores assigned by experts or criteria. Do they make sense or are they
high quality?

The quality of the approximation ($y = f(X)$) is only of importance if $y$ is
valid. If $y$ is trash then we get “a good approximation – but of trash”.
*Garbage in garbage out* it is that simple.

### Supervised learning (~predictive models)

**Supervised machine learning**: A function approximation approach that aims to
emulate a (unknown) mapping function from input data to labeled output values,
with the goal of accurately predicting the labels for new, unseen data.

**Supervised learning**: Develop an algorithm or predictive model that uses
labeled data $D:\{X,y\}_{i=1}^n$ to produces an accurate (e.g., non- or
piecewise linear) approximation of the mapping function “$f$”:  $y=f(X)$ -- and
make accurate predictions for future values of $\tilde{y}$ based on a set of
features ($X$). (*Note*: The labels are often assigned based on a criterion or
expert knowledge)

**Why is supervised learning “supervised”?**

Because the *training* data ($\mathcal{D}_{train}:\{X,y\}_{i=1}^{n_{train}}$)
you feed the algorithm includes the target values ($y$). Consequently, the
solutions can be used to help supervise the training process to find the
optimal algorithm parameters.

**Underlying function assumption**: Given a dataset comprised of inputs and
outputs $D:\{X,y\}_{i=1}^n$, we assume that there is an unknown underlying
function that is consistent in mapping inputs to outputs in the target domain
and resulted in the dataset. We then use supervised learning algorithms to
approximate this function.

*Open Question*: Does it need to be a function, or are also relations possible?
e.g. $y = x^2$?. I think relations are not possible, since there is no unique
element in the codomain or range for each element in the domain.

#### The two problems in supervised learning

1. Regression problem 
2. Classification problem

#### Regression problem

**Regression problem**: …means the objective of our supervised learning is to
predict a numeric outcome that falls on a continuum $(-\infty, \infty)$.

$$ y_{\in \mathbb{R}} = f(X_{\in \mathbb{R}}) $$

*Example*: Predict height based on persons characteristics, like sex.

#### Classification problem

**Clasification problem**: …means the objective of our supervised learning is
to predict a categorical outcome (i.e., a binary or multinomial response
measure)

*Example*: Classify customer reviews from 0 to 5 (multinomial) or “like”,
“don't like” (binary).

*Important*: In ML often ordinal responses are modeled as categorical processes
and thus with a multinomial distribution. 

**Classification problems are kind of regression problems**: Every
classification is a (logistic) regression problem. In a general regression
problem can be formalized as $y_{\in \mathbb{R}} = f(X_{\in \mathbb{R}})$. In a
classification problem the situation is as follows: $y_{\in [0,1]} = f(X_{\in
\mathbb{R}})$. *Note that $[0,1]$ is a subset of $\mathbb{R}^+$*. The problem
is thus: $y_{\in [0,1]\subset \mathbb{R}^+} = f(X_{\in \mathbb{R}})$. What
about the categories? In this binary case, the labels for the categories are
assigned ex post according to 0/1 assignment rule.

**0/1 Assignment rule**: If we predict the probability of a class ($P(X=1)$);
by default, the class with the highest predicted probability becomes the
predicted class:

$$ k = \begin{cases} 0 & \Leftrightarrow p < 0.5 \\ 1 & \Leftrightarrow p \geq
0.5 \end{cases} $$

### Unsupervised learning (~descriptive models)

**Unsupervised learning**: ...develop algorithms that uses unlabeled
information to discover functional relationships between variables (rows,
columns). 

In essence, unsupervised learning is concerned with identifying groups in a
data set. The groups may be defined by the rows (i.e., clustering) or the
columns (i.e., dimension reduction); however, the motive in each case is quite
different.

**How is unsupervised learning related to function approximation?**
Think of unsupervised learning as an emulator for a (unknown) function that
maps input data to some meaningful representation without any labeled output.

**Underlying function assumption**: Given a dataset: $D:\{Z_i\}$, we assume
that there is an unknown functional relationship between the variables (rows,
columns). We then use unsupervised learning algorithms to discover this
function.

#### The two topics in unsupervised learning:

1. Clustering (~ rows in the data set)
2. Dimension reduction (~ columns in the data set)

#### Clustering

**Clustering**: …means to use the observed variables (i.e., columns) to segment
observations (i.e., rows) into similar (more homogeneous) groups.

*Example*: “Market segmentation” -- where we divide consumers into different
homogeneous groups.

#### Dimension reduction

**Dimension reduction**: …means to reduce the number of variables (i.e.
columns) in a data set.

*Example*: Factor analysis -- identify the underlying dimensions (factors) that
explain patterns of correlations among multiple variables (items).

$$R^* =  \Lambda \Phi \Lambda + \Psi$$

##### Full compression vs. meaningful compression
Full -- orthogonal -- compression versus meaningful compression:

**Full compression**: If we have a regression model which tend to break with
highly correlated variables, or have a data set where the predictors are so
rich that we can't process them with our machines, we can use PCA to compress
the data set fully, i.e. reduce it with minimal loss in recovery to the
smallest amount of uncorrelated variables -- the principal components.

**Meaningful compression**:  If we want to understand or model the structural
relationships between variables with more flexibility (i.e. $\Phi$) we can try
to find factors that first and foremost make sense.

#### Downstream supervised learning

**Downstream supervised learning**: The outputs of unsupervised learning models
can be used as inputs to downstream supervised learning models.

*Example*: Using a reduced feature set of a PCA and input it to downstream
supervised learning models (e.g., principal component regression).

##### Problems: Quality assessment in supervised and unsupervised learning

With unsupervised learning it seems harder to assess the quality of the results,
because we don't know the true answer — the problem is unsupervised!

With predictive models and supervised learning algorithms (i.e., linear
regression), it is possible to check our work by seeing how well our model
predicts the response Y on observations not used in fitting the model.

However, this does not safeguard against the quality check for the labels. If
those are trash, then good predictions are meaningless.

### Reinforcement learning

**Reinforcement learning**: ...is a training method based on rewarding desired
behaviors and/or punishing undesired ones. In general, a reinforcement learning
agent is able to perceive and interpret its environment, take actions and learn
through trial and error.

## Chapter 2: Modeling Process

```{r}
# Helper packages
pkgs <- c("rsample", # Resampling procedures
          "caret",   # Resampling and model training 
          "h20")     # Resampling and model training
           
# lapply(pkgs, library, character.only=TRUE) 

# h2o 
h2o::h2o.no_progress() # Avoid the progress bars in the output
h2o::h2o.init()

?AmesHousing::ames_raw

# Ames housing data
ames <- AmesHousing::make_ames()

# Coerce to a h2o frame
ames_h2o <- h2o::as.h2o(ames)

# Job attrition data
# Note: the data are now in the modeldata package
churn <- modeldata::attrition
str(churn)
```

Important: h2o cannot handle ordered factors so we need to coerce them to 
unordered factors before the analysis.

```{r}
# Coerce ordered factors
unorder_if <- function(x) {
  # Have to call 'return()' explicitally
  ifelse(is.ordered(x), factor(x, ordered = FALSE), return(x))
  }
# Trick: Use '[]' to keep the data frame structure
churn[] <- lapply(churn, unorder_if)
str(churn)
```

**No free lunch theorem** (Wolpert 1996): "There is no single  model that can
outperform all other algorithms on all possible tasks or problems."

*Implication 1*: Participate in an iterative, playful and explorative model
building process. Never rely on a single "best" algorithm for all problems.
(see also: Stacking)

*Implication 2*: Approaching ML modeling correctly means approaching it
strategically by
* …spending our data wisely on learning and validation procedures,
* …properly pre-processing the feature and target variables,
* …minimizing data leakage,
* …tuning hyperparameters, and
* …assessing model performance

**Generalizability**: The ability of an algorithm to accurately predict future
outcomes, rather than just fitting well to past data, e.g. the sample.

### Data Splitting

To provide an accurate understanding of the generalizability of our final
optimal model, we can split our data into *training* and *test data sets*:

* **Training set**: The data we use to develop feature sets, train the
  algorithm, tune hyperparameters, and compare models.
* **Test set**: The data that we given a final model use to estimate an
  unbiased assessment of the model's performance on unseen data, i.e. its
  *generalization error*.

**Generalization error**: The difference between a model's performance on the
training data and its performance on new, unseen data, like the validation set.

#### Rough guidelines for the split

**N: small-medium**

* Spending too much in training (e.g., >80%) won't allow us to get a good
  assessment of predictive performance. We may find a model that fits the
  training data very well, but is not generalizable (*overfitting*).
* Sometimes too much spent in testing (>40%) won't allow us to get a good
  assessment of model parameters.

e.g. 60% (training)-40% (testing), 70%-30%, or 80%-20%

**N: big (100K)**

Small gains with additional data as compared to smaller sample sizes.

* $p \leq n$:  Use a smaller training sample to increase computation speed (e.g.,
  models built on larger training sets often take longer to score new data sets
  in production)
* $p \geq n$: Keep the samples sizes large, because the information is needed to
  identify consistent signals in the features.

#### Common splitting schemes:

1. Simple random sampling
2. Stratified sampling

#### Simple random sampling splitting

**Simple random sampling splitting**: Take a simple random sample from the 
data -- with the desired probability

**Simple random sampling**: A sampling method where each member of the
population has an equal and independent chance of being selected for the
sample. This means that every possible sample of the desired size has an equal
probability of being selected. 

*Procedure*: Assing each member of the population a unique identification
number, and use a random number generator to select the required number of
samples. 

*Benefits*: ...considered to be unbiased and used to minimize the risk of
selection bias, where certain subgroups of the population are overrepresented
or underrepresented in the sample. 

*Problem*: Does not control for any data attributes, such as the distribution
of your response variable ($y$).

*Usecases*: Classification and regression problems when we have a sufficient
sample size. In classification problems especially when there is a good balance
between the classes.

#### Simple random sampling

Use `base` and no additional packages.

```{r}
# Reproduceable results
set.seed(123)
# Number of rows
n_rows <- nrow(ames)
# Row index
row_index <- seq(n_rows)
# Index for the training cases
train_index <- sample(row_index, size = round(n_rows) * 0.7, replace = FALSE)
# Training set
train <- ames[train_index,] ; cat("Training set: ", dim(train), "\n")
# Testing set
test <- ames[-train_index,] ; cat("Testing set: ", dim(test), "\n")
# Plausibility check
cat("Split: ", dim(train)[1] / n_rows * 100, "/", dim(test)[1] / n_rows * 100)
```

Use the `caret` package.

```{r}
# Reproduceable results
set.seed(123)
# Index for the elements in the training set
train_index <- caret::createDataPartition(ames$Sale_Price, p = 0.7, 
                               # Matrix instead of a list as output
                               list = FALSE)
# Training set
train <- ames[train_index,] ; cat("Training set: ", dim(train), "\n")
# Testing set
test <- ames[-train_index,] ; cat("Testing set: ", dim(test), "\n")
# Plausibility check
cat("Split: ", dim(train)[1] / n_rows * 100, "/", dim(test)[1] / n_rows * 100)
```

Use the `rsample` package.

```{r}
# Reproduceable results
set.seed(123) 
split  <- rsample::initial_split(ames, prop = 0.7)
# Training set
train <- rsample::training(split)
# Testing set
test <- rsample::testing(split)
# Plausibility check
cat("Split: ", dim(train)[1] / n_rows * 100, "/", dim(test)[1] / n_rows * 100)
```

#### Stratified sampling splitting

**Stratified sampling splitting**: Take a stratified sample from the data --
with the desired probability.

**Stratified sampling**: A sampling method where the population is divided into
subgroups (aka., *strata*), and then a random sample is taken from each
subgroup (aka. *stratum*) in proportion to their size or importance. 

*Procedure*: Segment $y$ into classes (discrete $y$) or quantiles (continuous
$y$) and randomly sample from each.

*Benefits*: Allows to explicitly control the sampling so that our training and
test sets have similar Y distributions

*Usecases*: Classification and regression problems

1. *Classification problems*: When $y$ is severely imbalanced (e.g., 90% “Yes” and
   10% “No”). See also: Class Imbalances 
2. *Regression problems*: With small sample size and where y deviates strongly
   from normality (i.e., positively skewed like Sale_Price).

```{r}
# Data with class imbalances 
table(churn$Attrition) |> prop.table()
```

Stratified sampling using the `rsample` package.

```{r}
# Reproduceable results 
set.seed(123)
#  Split the data into training and testing sets
split_strat <- rsample::initial_split(churn, prop = 0.7, strata = "Attrition")
train <- rsample::training(split_strat) 
test <- rsample::testing(split_strat) 
# Plausibility check
cat("Training set", table(train$Attrition) |> prop.table()) 
cat("Testing set", table(test$Attrition) |> prop.table()) 
```

#### Class imbalances

Imbalanced data can have a significant impact on model predictions and
performance (Kuhn and Johnson 2013).

#### Categorization:

1. Up-sampling
2. Down-sampling
3. Over-Under-Sampling

#### Down-sampling

**Down sampling**: A technique used to address class imbalance by decreasing the number of instances in the majority class 

*Example*: Reducing the number of instances in the majority class to
achieve a balanced dataset by randomly selecting and removing a subset of
instances.

*Important*: By keeping all samples in the rare class and randomly selecting
an equal number of samples in the abundant class, a balanced new dataset can be
retrieved for further modeling. Furthermore, the reduced sample size reduces
the computation burden imposed by further steps in the ML process.

**When should we use down-sampling**? If there are enough data.

The option is implemented in the `caret`package. Use the `sampling` argument 
with the option `down`.

```{r}
?caret::trainControl
```

#### Up-sampling

**Up-sampling**: A technique used to address class imbalance by increasing the number of instances in the minority class 

*Example*:  Creating additional instances for the minority class by duplicating or synthesizing existing instances to address class imbalance

*Important*: Rather than getting rid of abundant samples, new rare samples are
generated by using repetition or bootstrapping.

**When should we use up-sampling**? When the quantity of data is insufficient.

The option is implemented in the `caret`package. Use the `sampling` argument 
with the option `up`.

```{r}
?caret::trainControl
```

#### Hybrid methods: Over-Under-Sampling

A combination of over- and under-sampling is often successful. A common
approach is known as *Synthetic Minority Over-Sampling Technique* (SMOTE,
Chawla et al. 2002).

**SMOTE**: ...generates synthetic samples of the minority class by
interpolating new instances between existing minority class instances. The
synthetic samples are created by randomly selecting a minority class instance,
selecting one or more of its k-nearest neighbors, and then creating a new sample
at a randomly selected point along the line segment joining the two instances.

**ROSE**: ...involves generating new synthetic examples for the minority class
by interpolating between existing examples, using a smoothed bootstrap approach
to sample from the feature space neighborhood around the minority class. The
resulting dataset has a balanced class distribution, allowing the model to
better learn patterns from the minority class.

*Note*: In their paper "Training and assessing classification rules with
imbalanced data" Giovanna Menardi and Nicola Torelli compare algorithms for
dealing class imbalances .

*ROSE vs. SMOTE*: It is often recommended to try multiple oversampling
techniques and evaluate their effectiveness using appropriate performance
metrics on a validation set.

SMOTE and ROSE are both implemented in the `caret` package. Use the `sampling`
argument with the option `smote` or `rose`. There is also the `DMwR` and the
`ROSE` package.

```{r, eval = FALSE}
?caret::trainControl
?DMwR::SMOTE() ; ?DMwR::ROSE()
help(package = "ROSE")
```

### Resampling methods

1. K-fold cross validation
2. Bootstrapping

#### K-fold cross validation

The purpose of cross-validation is to obtain a more robust estimate of the
model's performance by evaluating it on multiple validation sets. It helps to
assess how well the model generalizes to unseen data and provides insights into
its overall effectiveness.

**K-fold cross validation**: A (re-)sampling method that provides us with an
approximation of the generalization error, i.e. the error that we might expect
on unseen data.

A resampling method in which we divide the data set in k (equal) folds, train the model on k-1 of them and test it on the hold-out validation set in total k-times averaging the result to get an estimate of its generalizaition performance.

A sampling method includes sampling once from a data set and resampling to repeatedly sampling from the original data

*Procedure*: Randomly divide the training data into k groups (aka., folds) of
approximately equal size. Fit the model on k-1 folds and then compute the model
performance on remaining fold (aka. validation set). Repeating this procedure k
times (each time with a different validation fold) yields k estimates of the
generalization error. 

*Note:* The overall k-fold CV estimate is often computed by averaging the k
estimates of the generalization error. 

**The number of folds trade-off**: The larger $k$ the smaller the difference
between the estimated performance/generalization error and the true
performance/generalization error. I.e. Increasing $k$ helps to minimize the
variability in the estimated performance, However, there are computational
burdens.

**The number of folds**: ...often $k = 5$ or $k = 10$. Molinaro, Simon, and
Pfeiffer (2005): $k=10$ performed similarly to leave-one-out cross validation
(LOOCV) where $k=n$.

*Important*: Kim (2009) showed that repeating k-fold CV can help to increase
the precision of the estimated generalization error. Consequently, for smaller
data sets (say $n < 10,000$), 10-fold CV repeated 5 or 10 times will improve the
accuracy of your estimated performance and also provide an estimate of its
variability

*Note*: With multiple data sets, train the model on the combined data set.
This can be done with the `data.table` package. To stack rows of multiple
`data.tables` together, use the `rbindlist()` function.

CV as you can often perform CV directly within certain ML functions:

```{r, eval=FALSE}
# Example using h2o
h2o.cv <- h2o::h2o.glm(
  x = x, 
  y = y, 
  training_frame = ames.h2o,
  nfolds = 10  # perform 10-fold CV
)
```
You can alsp apply CV externally to an ML algorithm as below:

*Note:* When applying it externally to an ML algorithm as below, we'll need a
process to apply the ML model to each resample.

```{r}
rsample::vfold_cv(ames, v = 10)
```

#### Bootstrapping

**Bootstrap sample**: ...a random sample of the data taken with replacement
(Efron and Tibshirani 1986).

*Note*: Bootstrap sampling will contain approximately the same distribution of
values (represented by colors) as the original data set, especially
if $n_b \rightarrow n$.

**Out-of-bag (OOB) Observations**: Observations that are not included in in a
particular bootstrap sample.

*How can we use bootstrapping for validation*? Build a model on the selected
samples and validated it on the OOB samples; this is often done, for example,
in random forests (see Chapter 11).

*Procedure*: Draw a random sample (with replacement) from the full data set;
for example, $n_b = 0.8*n$. Keep the rest of the data, i.e. 20%, for
validation. Note that other splitting ratios are also possible (e.g., 70/30),
but obey the *Rough guidelines for the split* (see above). Repeat the procedure
b times to generate $b$ bootsrap samples. A thorough introduction to the
bootstrap and its use in R is provided in Davison, Hinkley, and others (1997).

*Note*: Bootstrapping is, typically, more of an internal resampling procedure
that is naturally built into certain ML algorithms, such as bagging and random
forests.

We can create bootstrap samples with the `rsample` package:

```{r}
# 80/20 split, b=10 bootstrap samples
rsample::bootstraps(ames, times = 10, size = 0.8)
```

#### Bootstrapping vs. K-fold CV

Since observations are replicated in bootstrapping, there tends to be less
variability in the error measure compared with k-fold CV (Efron 1983). However,
this can also increase the bias of your error estimate. This can be problematic
with smaller data sets; however, for most average-to-large data sets (say
$n \geq 1,000$) this concern is often negligible.

*Note*: The names "632 method" and "632+ method" are derived from the specific 
weighting scheme used in these techniques.

*Important*: For small datasets Efron (1983) developed the “632 method” and
Efron and Tibshirani (1997) discuss the “632+ method”; both approaches seek to
minimize biases experienced with bootstrapping on smaller data sets and are
available via `caret` (see the `boot632` argument in `caret::trainControl` for
details).

**632 Method**: A version of bootstrap validation where we address bias in smaller datasets by randomly assigning 63.2% (632) of the data for training and 36.8% for testing comparing the model's performance of the original data (in-sample) with its average performance of bootstrapped samples (OOS) 

```{r}
caret::trainControl(
  method = "boot632",
  number = 10,
  repeats = 5
)
```

**632+ Method**: An improved (version of the 632) bootstrap validation technique where we compare the reweighted model performance (x0.632) on the original data (in-sample) with its reweighted performance (x0.362) on the bootstrap samples (OOS).

##### Bootstrapping with time series

Hyndman and Athanasopoulos (2018) is the dominant, R-focused, time series
resource. 

*Note*: Hyndman published together with Talagala in 2022 a paper on
"meta-learning how to forecast time series". This is interesting because
time series is a nested data structure.

### Bias variance trade-off

We can decompose the prediction error into (1) an error due to "bias" and (2)
an error due to "variance". There is often a tradeoff between a model's ability
to minimize bias and variance. 

$$\text{Prediction error} = \text{Error due to bias} + \text{Error due to
variance}$$

#### Bias 

**Bias (error) function**: A function that measures the difference between the
expected (or average) model prediction and the correct value which we are
trying to predict. 

*Note*: This provides a sense of how well a model can conform to the underlying
structure of the data. The higher the bias the less it adapts to the underlying
structure - vice versa.

**Model complexity, Underfitting, and Bias**: Underfitting can result from
models with low degrees of freedom (i.e., few hyperparameters) that are not
flexible enough to capture the regular patterns in the data, leading to high
bias in predictions.

Put another way, (with) the model (we) impose(s) a strong prior belief about
the functional relationship between the input and output variables. If this
belief is false, the function approximation ($\hat f$) will be biased towards a
certain type of function ($\tilde f$) that does not match the underlying
function ($f$). The biased model (\hat f) won't fit the data well and make bad
predictions on new, unseen data.

*Consequences*: Bad predictions in-sample *and* out-of-sample (aka. high
generalization error).

*Example*: Linear models have high bias as they are less flexible and rarely
capture non-linear, non-monotonic relationships. 

*Counterexample*: Gradient Boosting Machines are low bias model as they are
flexible enough to capture non-linear, non-monotonic relationships. 

**Bias, Variance and Resampling**: Models with high bias - are often low
degrees of freedom models and thus - tend to underfit the data; they simply
ignore them. Consequently, high-bias model are rarely affected by the noise
introduced by resampling (e.g. on bootstrapped, or cross-validated data). They
have consistency (*low variance*) in their resampling performance 

#### Variance

**Variance (error) function**: A function that measures the variability of a
model prediction for a given data point. 

**Model complexity, Overfitting, and Variance**: Overfitting can result from
models with high degrees of freedom (i.e., many hyperparameters) that are too
flexible to capture only the regular patterns in the data. They also adapt to
irregular features (e.g., noise and outliers) leading to high variance in
predictions.

Put another way, (with) the model (we) impose(s) almost no prior belief about
the functional relationship between the input and output variables. With no
guidance, the model adapts to the only available source of information, the
data. Since every sample is an incomplete representation of the underlying
process, the function approximation will biased towards its artefacts thus will
not the underlying function ($f$) well. So even though the model (\hat f) will
fit the data (too) well it is doomed to make bad predictions on new, unseen
data.

*Consequences*: Good predictions in-sample *but* bad performance out-of-sample
(aka. high generalization error).

*Example**: K-nearest neighbor, Decision trees, gradient boosting machines are
very adaptable to non-linear data structures. However, they tend to overfit the
training data. 

**Bias, Variance and Resampling**: Since high variance models are often high
degrees of freedom model they are prone to overfitting. Resampling procedures
(e.g. bootstrapping, k-fold CV) are critical to reduce this risk. 

#### Navigating the bias-variance trade-off

Many algorithms that are capable of achieving high generalization performance
have lots of hyperparameters that *control* the level of model complexity (i.e.,
the tradeoff between bias and variance).

#### Hyperparameters

Hyperparameters (aka. tuning parameters) are the gear wheels to control the
complexity of machine learning algorithms. They can be used to balance bias and
variance and thus to navigate the bias-variance trade-off.

Note: Not all algorithms have hyperparameters (e.g., ordinary least squares8).

The proper setting of these hyperparameters is often dependent on the data and
problem at hand and cannot always be estimated by the training data alone.
Consequently, we need a method of identifying the optimal setting. 

Different approaches to performing grid searches:
1. Full Cartesian Grid Search
1. Random Grid Search 
1. Bayesian Optimization
1. Adaptive Resampling via Futility Analysis

##### Full Cartesian Grid Search

**Full cartesian grid search**: ...assesses every hyperparameter value manually
defined. 

*Problem*: As models get more complex and offer more hyperparameters, this
approach can become computationally burdensome and requires you to define the
optimal hyperparameter grid settings to explore. 

**Random Grid Search** (Bergstra and Bengio 2012): ...explores randomly
selected hyperparameter values from a range of possible values

*Problem*: Bad luck. Random grid search is not guaranteed to find the optimal
hyperparameter values.

**Early stopping**: ...allows you to stop a grid search once reduction in the
error stops marginally improving.

**Adaptive resampling via futility analysis** (Kuhn 2014): ...adaptively
resamples candidate hyperparameter values based on approximately optimal
performance, and more.

**Bayesian Optimization**: Bayesian optimization with a, let's say, Gaussian
process (GP) is a method for hyperparameter tuning that uses a probabilistic
model (a GP) to model the objective function (e.g., validation error) of a
machine learning model as a function of its hyperparameters. The model is
iteratively updated based on the evaluation of the objective function at
selected points, and a Bayesian approach is used to decide which point to
evaluate next based on a trade-off between *exploration* (trying new points to
improve the model) and *exploitation* (focusing on promising regions of the
hyperparameter space). This method is especially useful when the objective
function is expensive to evaluate, as it can lead to a more efficient search of
the hyperparameter space compared to grid search or random search.

#### Model evaluation

In the past, satistical models were mainly evaluated based on goodess-of-fit
measures (i.e., in-sample fit) and residual assessment. However, this approach
leads to misleading results (Breiman et al., 2001).

Today we agree on the better approach is on assesses *predictive accuracy*
(i.e., out-of-sample performance) via *loss functions*.

**Loss functions**: A function that compares the predicted values from the
model to the actual (target) value 

READ: https://www.statlect.com/glossary/loss-function

---

 In statistics and machine learning, a loss function quantifies the losses generated by the errors that we commit when:

   1. we estimate the parameters of a statistical model;

   2. we use a predictive model, such as a linear regression, to predict a variable.

The minimization of the expected loss, called statistical risk, is one of the guiding principles in statistical modelling. 

#### Estimation loss

**Estimation error**: If we use some data to produce an estimate $\hat \beta$
of the unknown $beta$, we will end up with $d(\beta, \hat beta) > 0$. This
difference between the estimate and the true value is called the estimation
error.

**Estimation loss function**: Our wish to minimize the estimation error, can be
formulated using a loss function $L(\hat \beta, \beta)$; which measures the
loss incurred when we estimate $\beta$ with $\hat \beta$.

*Example*: When $\beta$ is a scalar, the quadratic loss is: $L(\beta, \hat
\beta) = (\beta - \hat \beta)^2$.

#### Prediction loss

**Prediction error**: If we've estimated a, let's say regression, model, we can
compare its predicitons $\hat y$ to the true values $y$, the $d(y, \hat y) > 0$,
where $\hat y = X \hat \beta$. This difference between the prediction and the
true value is called the prediction error.

**Estimation loss function**: Our wish to minimize the prediction error, can be
formulated using a loss function $L(\hat y, y)$; which measures the
loss incurred when we predict $y$ with $\hat y$

*Important*: Most of the functions that are used to quantify prediction losses
are also used for estimation losses. 

#### Risk and empirical risk

**Risk**: The expected value of the loss is called risk.

**Estimator**: A random variable whose realization is equal to the estimate

**Risk of the estimator**: When $\widehat{\beta }$ is seen as an estimator,
(i.e., A random variable whose realization is equal to the estimate) the
expected value $\mathrm E[L(\hat \beta, \beta)]$, is the *risk of the
estimator*.

**Risk of the predictions**: The expected value $\mathrm E[L(\hat y, y)]$, is
the risk of the predictions

**Empirical risk** The risk of the predictions can be approximated by the
empirical risk, its sample counterpart: $\frac{1}{N}\sum_{i=1}^NL(\hat y_i,
y_i)$, where $N$ is the sample size.

#### Prediction & estimation risk (TODO)

The prediction risk depends on the estmation risk. It is thus at least as big
as the estimation risk $L(\hat \beta, \beta) \leq L(\hat y, y)$. This can be
seen given the quadratic loss, for instance:

$$\mathbb E[L(\hat y_i, y_i)] = \mathbb E(L(X_i \hat \beta, y_i) = \mathbb
E(L(X_i (\sqrt{L(\hat \beta, \beta)} + \beta), y_i)$$

#### Empirical risk minimization

In linear regression the vector of regression cofficients $\mathbf \beta$ is
usually estimated by empirical risk minimization.

The predictions $\hat y_i$ depend on $\hat beta$, and so does the empirical
risk. We search for the vector $\hat \mathbf \beta$ that minimizes the empirical
risk.

 The Ordinary Least Squares (OLS) estimator of $\hat\beta$ is the empirical
 risk minimizer when the quadratic loss (details below) is used as the loss
 function. In fact, the OLS estimator solves the minimization problem:

$$\hat \beta = \arg \min_{\beta \in \mathbb R^p} \sum_{i=1}^N L(\hat y_i - y_i)$$

$$\hat \beta = \arg \min_{\beta \in \mathbb R^p} \sum_{i=1}^N (\hat y_i - y_i)^2$$

$$\hat \beta = \arg \min_{\beta \in \mathbb R^p} \sum_{i=1}^N (y_i - X_i \beta)^2$$

Under the conditions stated in the Gauss-Markov theorem, the OLS estimator is
also the unbiased estimator that generates the lowest expected estimation
losses, provided that the quadratic loss is used to quantify the latter. 

---

*Note*: The output of a loss function is often referred to as the *error* or
*pseudo residual*. 

*Example*: In OLS Regression the prediction error is: $y_i - \hat y_i$, where
\hat y_i = X_i \beta$, and thus the loss function is: $L(y_i - \hat y_i)$ 

Objective: minimize


For example, in regression, one way to measure error is to take the difference
between the actual and predicted value for a given observation (this is the
usual definition of a residual in ordinary linear regression). 

The overall validation error of the model is computed by aggregating the errors
across the entire validation data set.

There are many loss functions to choose from when assessing the performance of
a predictive model, each providing a unique understanding of the predictive
accuracy and differing between regression and classification models.

Furthermore, the way a loss function is computed will tend to emphasize certain
types of errors over others and can lead to drastic differences in how we
interpret the “optimal model”. Its important to consider the problem context
when identifying the preferred performance metric to use. And when comparing
multiple models, we need to compare them across the same metric.

2.6.1 Regression models

MSE: Mean squared error is the average of the squared error
(MSE=1n∑ni=1(yi−^yi)2)9. The squared component results in larger errors having larger penalties.
This (along with RMSE) is the most common error metric to use. 

Objective: minimize

RMSE: Root mean squared error. This simply takes the square root of the MSE
metric (RMSE=√1n∑ni=1(yi−^yi)2) so that your error is in the same units as your
response variable. If your response variable units are dollars, the units of
MSE are dollars-squared, but the RMSE will be in dollars. Objective: minimize

Deviance: Short for mean residual deviance. In essence, it provides a degree to
which a model explains the variation in a set of data when using maximum
likelihood estimation. Essentially this compares a saturated model (i.e. fully
featured model) to an unsaturated model (i.e. intercept only or average). If
the response variable distribution is Gaussian, then it will be approximately
equal to MSE. When not, it usually gives a more useful estimate of error.
Deviance is often used with classification models.10 Objective: minimize

MAE: Mean absolute error. Similar to MSE but rather than squaring, it just
takes the mean absolute difference between the actual and predicted values
(MAE=1n∑ni=1(|yi−^yi|)). This results in less emphasis on larger errors than
MSE. Objective: minimize

RMSLE: Root mean squared logarithmic error. Similar to RMSE but it performs a
log() on the actual and predicted values prior to computing the difference
(RMSLE=√1n∑ni=1(log(yi+1)−log(^yi+1))2). When your response variable has a wide
range of values, large response values with large errors can dominate the
MSE/RMSE metric. RMSLE minimizes this impact so that small response values with
large errors can have just as meaningful of an impact as large response values
with large errors. Objective: minimize

R2: This is a popular metric that represents the proportion of the variance in
the dependent variable that is predictable from the independent variable(s).
Unfortunately, it has several limitations. For example, two models built from
two different data sets could have the exact same RMSE but if one has less
variability in the response variable then it would have a lower R2
    than the other. You should not place too much emphasis on this metric. Objective: maximize

Most models we assess in this book will report most, if not all, of these
metrics. We will emphasize MSE and RMSE but it’s important to realize that
certain situations warrant emphasis on some metrics more than others. 2.6.2
Classification models

Misclassification: This is the overall error. For example, say you are
predicting 3 classes ( high, medium, low ) and each class has 25, 30, 35
observations respectively (90 observations total). If you misclassify 3
observations of class high, 6 of class medium, and 4 of class low, then you
misclassified 13 out of 90 observations resulting in a 14% misclassification
rate. Objective: minimize

Mean per class error: This is the average error rate for each class. For the
above example, this would be the mean of 325,630,435, which is 14.5%. If your
classes are balanced this will be identical to misclassification. Objective:
minimize

MSE: Mean squared error. Computes the distance from 1.0 to the probability
suggested. So, say we have three classes, A, B, and C, and your model predicts
a probability of 0.91 for A, 0.07 for B, and 0.02 for C. If the correct answer
was A the MSE=0.092=0.0081 , if it is B MSE=0.932=0.8649, if it is C
MSE=0.982=0.9604. The squared component results in large differences in
probabilities for the true class having larger penalties. Objective: minimize

Cross-entropy (aka Log Loss or Deviance): Similar to MSE but it incorporates a
log of the predicted probability multiplied by the true class. Consequently,
this metric disproportionately punishes predictions where we predict a small
probability for the true class, which is another way of saying having high
confidence in the wrong answer is really bad. Objective: minimize

 Gini index: Mainly used with tree-based methods and commonly referred to as a
 measure of purity where a small value indicates that a node contains
 predominantly observations from a single class. Objective: minimize

When applying classification models, we often use a confusion matrix to
evaluate certain performance measures. A confusion matrix is simply a matrix
that compares actual categorical levels (or events) to the predicted
categorical levels. When we predict the right level, we refer to this as a true
positive. However, if we predict a level or event that did not happen this is
called a false positive (i.e. we predicted a customer would redeem a coupon and
they did not). Alternatively, when we do not predict a level or event and it
does happen that this is called a false negative (i.e. a customer that we did
not predict to redeem a coupon does).

We can extract different levels of performance for binary classifiers. For example, given the classification (or confusion) matrix illustrated in Figure 2.13 we can assess the following:

    Accuracy: Overall, how often is the classifier correct? Opposite of misclassification above. Example: TP+TNtotal=100+50165=0.91

. Objective: maximize

Precision: How accurately does the classifier predict events? This metric is concerned with maximizing the true positives to false positive ratio. In other words, for the number of predictions that we made, how many were correct? Example: TPTP+FP=100100+10=0.91

. Objective: maximize

Sensitivity (aka recall): How accurately does the classifier classify actual events? This metric is concerned with maximizing the true positives to false negatives ratio. In other words, for the events that occurred, how many did we predict? Example: TPTP+FN=100100+5=0.95

. Objective: maximize

Specificity: How accurately does the classifier classify actual non-events? Example: TNTN+FP=5050+10=0.83
. Objective: maximize


AUC: Area under the curve. A good binary classifier will have high precision
and sensitivity. This means the classifier does well when it predicts an event
will and will not occur, which minimizes false positives and false negatives.
To capture this balance, we often use a ROC curve that plots the false positive
rate along the x-axis and the true positive rate along the y-axis. A line that
is diagonal from the lower left corner to the upper right corner represents a
random guess. The higher the line is in the upper left-hand corner, the better.
AUC computes the area under this curve. Objective: maximize


### MISC

*Stupid question*: Does Random Forest use optimization algorithms such as SGD?
No, it an ensemble method that uses decision trees as base learners, and the
splitting criterion used in decision trees is usually based on the Gini
impurity or information gain. The process of constructing decision trees
involves finding the best split point at each node, which can be seen as an
optimization problem. However, once the decision trees are constructed, no
specific optimization algorithm is used for combining them in the Random Forest
algorithm. Instead, a deterministic algorithms (e.g. the mean) is used to
average or combine the predictions of the individual decision trees.

*Stupid question*: Does Gradient Boosting use optimization algorithms such as
SGD? Yes it uses an optimization algorithm, but for optimization is does not
use SGD. However, it is based on the concept of gradient descent. Instead of
minimizing the loss function directly as in SGD, Gradient Boosting iteratively
fits weak learners to the negative gradient of the loss function with respect
to the current model predictions. This is a similar optimization approach to
gradient descent, but it is not stochastic in nature. Therefore, Gradient
Boosting is typically considered a gradient-based optimization algorithm,
rather than a stochastic optimization algorithm.



Note to self: Is this why a stack of GBMs performs better than a single GBM?
Idea: if we combine a lot of high variance, low bias models, and then use a DNN
to regularize as meta-model, we should be good to go. DNNs can stronger
regularize than LASSO, Ridge, etc. Way better would be a DNN optimized by a GP
or a GP or a Stack of DNNs


h2o algorithms have a weights_column and balance_classes

Many of these drawbacks and inconsistencies were originally organized and
presented by Kuhn (2018)

The caret package has been the preferred meta engine over the years; however,
the author is now transitioning to full-time development on parsnip, which is
designed to be a more robust and tidy meta engine.↩

### TODO Digression: Are BDA and ML separate things?

I really try to see ML algorithms as function approximation machines. Often,
they provide (smoothed) point-wise linear (e.g., DNNs) or non-linear (e.g.,
GPs) approximations of underlying (non-linear) functions.

However, for me it was hard to see how machine learning algorithms can be used
in probabilisitc modeling more generally and how they interplay with multilevel
models, for instance. I thought machine leanring and, what I call, "Bayesian
data" analysis are two different worlds. They are not. Both can be embedded in
framework of probabilisitc modeling; and thus we can combine then to produce
even more powerful models. 

*Example*: Combine GPs and Hierarchical modeling. One way to implement this
approach is to use a Bayesian hierarchical model, where the GP is used as the
link function for the regression model at each level. The parameters of the GP
are then estimated using a Bayesian approach, which allows for uncertainty
quantification in the predictions. Overall, using GPs to approximate the link
function in a multilevel model can provide a flexible and probabilistic
approach to modeling non-linear relationships between variables.

### Digression: Deep Neural Nets ~ Gaussian Processes

Lee, Bahri, et al. (2018) showed that, if the number or nodes in a carefully
scaled NN (with a single hidden layer) -- think of a MLP -- converges to
infinity, a DNN converges to a GP. However, the argument can be extended to
deeper layers by induction.

**GPs > DNN**: Consequently, *GPs only emulate non-linearity*. To put it
another way, GPs kind of resolve the parametric assumptions that MLPs make.
So I would prefer to use a GP over a MLP whenerver possible, even if your
sample size grows infinitely large. There are tricks to get it going; if not,
use a DNN.

**DNN > GPs**: Since the computational cost of GPs naively scales as
$\mathcal{O}(N^3)$, where $N$ are the data points, consider a MLP if a GP is
not possible with your data.

*Open Question*: I think this holds only for feed forward neural nets such as
MLPs. Recurrent Neural Networks (RNNs) and Convolutional Neural Networks
(CNNs), for instance, can be used to model time series data and image data,
respectiively, and are not limited to pointwise linear function approximation.


~ Fourier series, Taylor expansion?

As many pointed out, a regression/decision tree is a non-linear model. Note
however that it is a piecewise linear model: in each neighborhood (defined in a
non-linear way), it is linear. In fact, the model is just a local constant.

Note: RF are tree based, while DNNs are gradient based.

https://machinelearningmastery.com/neural-networks-are-function-approximators/

Training a neural network on data approximates the unknown underlying mapping
function from inputs to outputs.

Is this why we can use a DNN to learn a link function?

Note: Deep Learning Is Not 'Worse' than Trees on Tabular Data
https://towardsdatascience.com/deep-learning-is-not-worse-than-trees-on-tabular-data-1de25ed31d2

--

(Non-)parametric

Nonparametric regression: is a category of regression analysis* in which the
predictor does not take a predetermined form but is constructed according to
information derived from the data. More specifically, the data supply the model
structure as well as the model estimates.

Q: is the number of parameters increasing as you process new training examples
during training? If so, the method is non-parametric.

Example: Gaussian process regression

*Regression analysis is a model family where the Data are divided in D:{X,y}
and we estimate the relationship “f” between X and y: y=f(X)

Are standard deep neural networks (DNN) parametric?

Technically speaking, yes, since they have a fixed number of parameters. (We
have to determine the model structure, I.e. layers, neurons, etc. beforehand)

Deep learning models are generally parametric - in fact they have a huge number
of parameters, one for each weight that is tuned during training. As the number
of weights generally stays constant, they technically have fixed degrees of
freedom. However, as there are generally so many parameters they may be seen to
emulate non-parametric.

Are Gaussian processes non-parametric?
Technically speaking, yes, since

GPs are non-parametric since it the number of parameter increase he number of
parameters increasing as you process new training examples – O(N^3)

Gaussian processes (for example) use each observation as a new weight and as
the number of points goes to infinity so too do the number of weights (not to
be confused with hyper parameters). I say generally because there are so many
different flavours of each model. For example low rank GPs have a bounded
number of parameters which are inferred by the data and I'm sure someone has
been making some type of non-parametric dnn at some research group!

—

Non-linear and non-parametric

(My idea – In a nutshell): in regression, non-linear refers to the shape of the
outcome y - variable, parametric to the way the X are combined.

—
